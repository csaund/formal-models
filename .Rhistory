install.packages("statnet")
install.packages("ndtv")
install.packages("MASS")
install.packages("numderiv")
install.packages("numDeriv")
setwd("~/Uni/Postgraduate/Network Analysis/SNAWorkshop")
###########################################################
###########################################################
##2 A Brief R Tutorial
##2.1 Introduction to basic R syntax
1 + 3 # evaluation
library(statnet)
list.files()
#Read an adjacency matrix (R stores it as a data frame by default)
relations <- read.csv("relationalData.csv",header=FALSE,stringsAsFactors=FALSE)
relations
View(relations)
#Here's a case where matrix format is preferred
relations <- as.matrix(relations) # convert to matrix format
#Read in some vertex attribute data (okay to leave it as a data frame)
nodeInfo <- read.csv("vertexAttributes.csv",header=TRUE,stringsAsFactors=FALSE)
nodeInfo
#Since our relational data has no row/column names, let's set them now
rownames(relations) <- nodeInfo$name
colnames(relations) <- nodeInfo$name
relations
nrelations<-network(relations,directed=FALSE) # Create a network object based on relations
nrelations # Get a quick description of the data
nempty <- network.initialize(5) # Create an empty graph with 5 vertices
nempty # Compare with nrelations
edgelist<-read.csv("edgeList.csv",header=T,stringsAsFactors = F)
head(edgelist)
edgeNet<-network(edgelist,matrix.type="edgelist")
edgeNet
plot(edgeNet,displaylabels=T) ##what's missing?
edgeNet<-network(edgelist,matrix.type="edgelist",ignore.eval=F,names.eval="weight")
edgeNet
plot(edgeNet,displaylabels=T,edge.lwd=5*edgeNet%e%"weight")
plot(edgeNet,displaylabels=T,edge.lwd=10*edgeNet%e%"weight")
data(emon) # Load Drabek et al. data
View(emon)
class(emon[[1]])
emon[[1]]
emon[[1]]%v%"vertex.names" # Display vertex names
# Extract ties from the Cheyenne EMON communicating at least "every few hours"
g<-as.sociomatrix(emon[[1]],"Frequency") # Need to get the frequency info
g<-symmetrize((g>0)&(g<4)) # Note the reverse coding!
#Get some potential covariates
drs<-emon[[1]]%v%"Decision.Rank.Score" # Get decision rank (see man page)
crs<-emon[[1]]%v%"Command.Rank.Score" # Get command rank
# Plot Cheyenne EMON communications
gplot(emon[[1]])
gplot(emon[[1]], label.cex=0.5, label.col=4, label=network.vertex.names(emon[[1]])) # Basic display, with labels
#Calculate some basic centrality measures
deg<-degree(g,gmode="graph")
bet<-betweenness(g,gmode="graph")
clo<-closeness(g,gmode="graph")
#Raw correlations
cor(cbind(deg,bet,clo),cbind(drs,crs))
#Classical tests (using asymptotic t distribution)
cor.test(deg,drs)
cor.test(bet,drs)
cor.test(clo,drs)
###########################################################
#5.2 Testing correlations
#Lets build a permutation test
deg
drs
c.obs<-cor(deg,drs)
c.obs
#Permute one of the data sets
sample(drs)
cor(deg,sample(drs))
#write a for loop to permute one of the data sets many times
c.rep<-vector(length=1000)
for(n in 1:1000){
c.rep[n]<-cor(deg,sample(drs))
}
#look at what we've created
c.rep
hist(c.rep)
#compare to empirical data
abline(v=c.obs,col="red")
#write a for loop to permute one of the data sets many times
c.rep<-vector(length=10000)
for(n in 1:10000){
c.rep[n]<-cor(deg,sample(drs))
}
#look at what we've created
c.rep
hist(c.rep)
#compare to empirical data
abline(v=c.obs,col="red")
###########################################################
#5.4 Graph correlation and bivariate QAP
# Remember the Florentine families data
data(florentine)
gplot(flobusiness) # Examine business ties
gplot(flomarriage) # Examine marriage ties
par(mfrow=c(1,2))
coords<-plot(flobusiness,label=flobusiness%v%"vertex.names",label.cex=.5,pad=1)
title("Business Ties")
plot(flomarriage,label=flomarriage%v%"vertex.names",label.cex=.5,pad=1,coord=coords)
title("Marriage Ties")
par(mfrow=c(1,1))
# Let's try a graph correlation
gcor(flobusiness,flomarriage)
# Why can't we use our previous permutation test?
# instead, use rmperm
# take a look
rmperm
par(mfrow=c(1,2))
gplot(flobusiness[,],label=flobusiness%v%"vertex.names",label.cex=.5,pad=1,coord=coords)
title("Business Ties")
gplot(rmperm(flobusiness),label=flobusiness%v%"vertex.names",label.cex=.5,pad=1,coord=coords)
title("Permuted Business Ties")
par(mfrow=c(1,1))
#now look at qaptest
qaptest
#and use it
qt<-qaptest(list(flobusiness,flomarriage),gcor,g1=1,g2=2)
summary(qt) # Examine the results
plot(qt) # Plot the QAP distribution
# Testing against covariate effects
wealth<-sapply(flomarriage%v%"wealth",rep,network.size(flomarriage))
wealth
wealthdiff<-abs(outer(flomarriage%v%"wealth",flomarriage%v%"wealth","-"))
wealthdiff
qt1<-qaptest(list(flomarriage,wealth),gcor,g1=1,g2=2)
qt2<-qaptest(list(flomarriage,wealthdiff),gcor,g1=1,g2=2)
summary(qt1) # Do wealthy families have more ties?
summary(qt2) # Is there a wealth difference effect?
# Combine the previous tests (takes a while to perform QAP test)
marriagefit<-netlm(flomarriage,list(flobusiness,wealth,wealthdiff))
summary(marriagefit) # Examine the results
load("SNAworkshopData.Rdata")
rownames(parishNet)
which(parishNet["Rapides",]==1)
head(parishAttr)
modLM<-lm(parishAttr[,1]~parishAttr[,2]+parishAttr[,3])
summary(modLM)
setwd("~/Uni/Postgraduate/Research Project/Data")
knitr::opts_chunk$set(echo = TRUE)
dat <- read_csv("19.2.2019.csv")
library(tidyverse)
dat <- read_csv("19.2.2019.csv")
View(dat)
dat <- read_csv("19.2.2019.csv") %>% select("session_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv")
View(dat)
knitr::opts_chunk$set(echo = TRUE)
dat <- read_csv("6.2.2019 testing wrangling.csv") %>%
select("session_id","quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv")
View(dat)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  #filter out q_name NAs (testing runs)
View(dat)
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
library(tidyverse)
ocean <- read_csv("https://gupsych.github.io/data_skills/data/personality.csv") %>%
gather(question, score, Op1:Ex9)
View(ocean)
eyes <- read_csv("https://gupsych.github.io/data_skills/data/eye_descriptions.csv")
View(eyes)
eyes <- read_csv("https://gupsych.github.io/data_skills/data/eye_descriptions.csv") %>%
gather(face_id, description,t1:t50)
## convert to a tibble
ukb <- as_tibble(ukbabynames)
library("ukbabynames")
library("lubridate")
## convert to a tibble
ukb <- as_tibble(ukbabynames)
View(ukb)
View(dat)
count(dat, user_id)
dat %>% distinct(user_id) %>% count()
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  #filter out q_name NAs (testing runs)
View(dat)
filter(dat,q_name = consent)
filter(dat,q_name = "consent")
filter(dat,q_name == "consent")
consent <- filter(dat,q_name == "consent")
View(consent)
consent <- filter(dat,q_name == "consent", "debrief")
consent <- filter(dat,q_name == c("consent", "debrief"))
View(consent)
View(consent)
consent <- filter(dat,q_name == "consent")
View(consent)
View(consent)
debrief <- filter(dat,q_name == "debrief")
View(debrief)
View(dat)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
filter(q_name == "consent" & dv == 0)
View(dat)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
filter(q_name == "consent" & dv == 1)
View(dat)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
filter(q_name == "consent", dv == 1)
library("ukbabynames")
## convert to a tibble
ukb <- as_tibble(ukbabynames)
filter(ukb, name == "Hermione")
filter(ukb, name == "Courtney", sex =="M", year >= 1998, year <= 2001)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
filter(q_name == "consent", dv == 1) #nope... only selects those two conditions, I want to combine them but have other vars too
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
filter(q_name == "consent", dv == 1) #nope... only selects the "consent" question, I want to combine them but have other questions too
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))  %>% #filter out q_name NAs (testing runs)
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))
dat <- read_csv("19.2.2019.csv") %>%
select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
filter(!is.na(q_name))
View(dat)
?spread
dat_q_spread <- dat %>%
spread(quest_id, dv)
View(dat_q_spread)
rename()
?rename
dat_q_spread <- dat %>%
spread(quest_id, dv) %>% rename(479 = sport, 478 = demographics, 372 = OSM)
dat_q_spread <- dat %>%
spread(quest_id, dv) %>% rename(479, sport)
?spread
dat_q_spread <- dat %>%
spread(quest_id, dv) %>% rename(479="sport")
dat_q_spread <- dat %>%
spread(quest_id, dv) %>% rename(479=sport)
dat_q_spread <- dat %>%
spread(quest_id, dv) %>% rename(479, sport)
?names
dat_q_spread <- dat %>%   #spread based on questionnaire id, maybe then I can filter out consent (467)...?
spread(quest_id, dv) %>%
filter(467 != 0)
dat_q_spread <- dat %>%   #spread based on questionnaire id, maybe then I can filter out consent (467)...?
spread(quest_id, dv) %>%
filter(467 != 0)
dat_q_spread <- dat %>%   #spread based on questionnaire id, maybe then I can filter out consent (467)...?
spread(quest_id, dv)
ess <- dat_q_spread %>% group_by(user_id) %>% select(244)
ess <- dat_q_spread %>% group_by(user_id) %>% select("244")
View(ess)
ess <- dat_q_spread %>% group_by(user_id) %>% filter(q_name == ess_1:ess_8)
ess <- dat_q_spread %>% group_by(user_id) %>% filter(q_name == ess_1)
ess <- dat_q_spread %>% group_by(user_id) %>% filter(q_name == "ess_1")
ess <- dat_q_spread %>% group_by(user_id) %>% filter(q_name == c("ess_1", "ess_2"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
raw_dat <- read_csv("19.2.2019.csv")
#%>%
# select("session_id", "quest_id", "user_id", "user_status", "user_age", "quest_id", "q_name", "dv") %>%
#filter(!is.na(q_name)) #filter out q_name NAs (testing runs)
#dat_q_spread <- dat %>%   #spread based on questionnaire id, maybe then I can filter out consent (467)...?
# spread(quest_id, dv) %>%
# filter()
#cannot rename, not sure why
# filter(q_name == "consent", dv == 1) #nope... only selects the "consent" question, I want to combine them but have other questions too
#ideas: %in%   all()   - don't understand what they're doing
#consent <- filter(dat,q_name == "consent")
#debrief <- filter(dat,q_name == "debrief") #those who got all the way to the debrief
View(raw_dat)
clean_dat <- raw_dat %>%
select(user_id, user_sex, user_status, user_age, quest_id, q_name, dv) #%>%
filter(!is.na(q_name)) #filter out q_name NAs (testing runs)
View(clean_dat)
clean_dat <- raw_dat %>%
select(user_id, user_sex, user_status, user_age, quest_id, q_name, dv) %>%
filter(user_status == "registered")
RT.data <- read.csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
View(RT.data)
setwd("~/Uni/Postgraduate/Statistics")
RT.data <- read.csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
head(RT.data)
x - RT.data$logfreq
x <- RT.data$logfreq
y <- RT.data$RT
plot(x, y,
xlab = "log lex. frequency",
ylab = "RT(ms)")
# Calculate intercept (beta_0) and
# slope (beta_1) "on foot"
slope <- cor(x, y) * (sd(y) / sd(x))
intercept <- mean(y) - slope * mean(x)
# Look at what we’ve done
c("beta_0" = intercept, "beta_1" = slope)
LF1 <- lm(y ~ x)
summary(LF1)
#alternatively
LF2 <- lm(RT ~ logfreq, data=RT.data)
summary(LF2)
RT.data$freqCAT <- ifelse(RT.data$logfreq > median(RT.data$logfreq),
"high", "low")
head(RT.data)
# "Dummy coding" of freqCAT
RT.data$freqCAT_dummy <- ifelse(RT.data$freqCAT == "low",0,1)
# Perform linear regression (RT as a function of freqCAT_dummy)
catmod <- lm(RT ~ freqCAT_dummy, data = RT.data)
summary(catmod)
knitr::opts_chunk$set(echo = TRUE)
dat <- read_csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
View(dat)
View(dat)
plot <- ggplot(dat, aes(logfreq, RT)) + geom_jitter(aes(spelling))
View(plot)
plot <- ggplot(dat, aes(spelling)) + geom_jitter()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
dat <- read_csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
plot <- ggplot(dat, aes(spelling)) + geom_jitter()
View(plot)
plot <- dat %>%  ggplot(aes(spelling)) + geom_jitter()
plot <- dat %>%  ggplot() + geom_jitter(aes(spelling))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
dat <- read_csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
plot <- dat %>%  ggplot() + geom_jitter(aes(spelling))
View(plot)
plot <- ggplot(dat, aes(x= logfreq, y= RT)) +
geom_point()
library(ggplot2)
dat <- read_csv("http://www.psy.gla.ac.uk/~christop/MScStats/2018/Regress/RTs.csv")
plot <- ggplot(dat, aes(x= logfreq, y= RT)) +
geom_point()
